{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q\n",
    "!pip install fasttext -q\n",
    "from transformers import Trainer, TrainingArguments, RobertaForMultipleChoice, RobertaTokenizerFast, XLMRobertaForMultipleChoice, XLMRobertaTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score \n",
    "import copy\n",
    "from transformers.tokenization_utils_base import PaddingStrategy, TruncationStrategy\n",
    "from prettytable import PrettyTable\n",
    "import csv\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import os\n",
    "import fasttext\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./coveo/train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDatset(Dataset):\n",
    "    \n",
    "    \"\"\"This class create the dataset needed to train a multiplechoice Model.\n",
    "    It gets a panda dataframe and a tokenizer,which gets the text and the choices and provides \n",
    "    the tokenized input in the shape of [number_of_choices, length_of_the_text] in following format \n",
    "\n",
    "                    <s>text</s></s>choice<lang-identifier></s><pad> \n",
    "                    \n",
    "    and it provides a python Dataset\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        dataframe: a panda dataframe containing the examples\n",
    "        tokenizer_name: Tokenizer model name. Default is 'Longformer'\n",
    "        tokenizer_pth: Path to load the tokenizer from.  Default is 'allenai/longformer-base-4096'\n",
    "        mode: A string, 'train' or 'test', to determine if the dataset has labels or not. Default is 'train'\n",
    "        max_length: The longest possible sequences. the default value is 512. \n",
    "        seq_length: The sequence length from trimming the input text.\n",
    "        lang_identifier: if set True, it will add a language identifier to each choice. default value is False\n",
    "    \n",
    "    \n",
    "    Return:\n",
    "    \n",
    "        A python dataset, which returns a dictionary containing Input_ids, Attention_mask and Label.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe: pd.DataFrame, tokenizer_name: str='Roberta',\n",
    "                 tokenizer_pth: str= 'roberta-base', mode: str='train',\n",
    "                 max_length: int =512, seq_length: int=200, lang_identifier: bool=False):\n",
    "        \"\"\"Read the dataframe row by row, tokenized them and store them in a list\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "            dataframe:  panda datafrmae\n",
    "            tokenizer_name\n",
    "            tokenizer_pth\n",
    "            mode\n",
    "        \n",
    "        Attributes:\n",
    "            \n",
    "            self.instances: a list to store the instances in the dataframe after tokenization as\n",
    "            a tuple (tokenized_input, label)\"\"\"\n",
    "        \n",
    "        \n",
    "        tokenizer_models = {'XLM' : XLMRobertaTokenizerFast,\n",
    "                           'Roberta': RobertaTokenizerFast\n",
    "                           }\n",
    "        \n",
    "        tokenizer = tokenizer_models[tokenizer_name].from_pretrained(tokenizer_pth)\n",
    "        orig_length = len(tokenizer)\n",
    "        num_added_tokens = 0\n",
    "        self.lang = lang_identifier\n",
    "        if self.lang:\n",
    "            self.fr_token = '<fr_lang>'\n",
    "            self.en_token = '<en_lang>'\n",
    "            self.other_token = '<other_lang>'\n",
    "            special_tokens_dict = {'additional_special_tokens': [self.fr_token,self.en_token,self.other_token]}\n",
    "            num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "            self.lang_detector = fasttext.load_model('./coveo/lid.176.bin')\n",
    "            \n",
    "        self.total_num = orig_length + num_added_tokens\n",
    "        \n",
    "\n",
    "        \n",
    "        self.instances = []\n",
    "        self.seq_length = seq_length\n",
    "        self.mode = mode\n",
    "        bar = tqdm(total = len(dataframe))\n",
    "        for index, row in dataframe.iterrows():\n",
    "            choices = self.extract_choices(row)\n",
    "            text = self.trim_text(row['text'], tokenizer, seq_length=self.seq_length)\n",
    "            tokenized_input = tokenizer([text]*len(choices), choices, padding=True,\n",
    "                                        max_length=max_length, return_tensors='pt')\n",
    "\n",
    "            \n",
    "            if self.mode == 'train' or self.mode == 'validation':\n",
    "                label = [self.get_label(row['label'])]\n",
    "            else:\n",
    "                label = ''\n",
    "            self.instances.append((tokenized_input, label))\n",
    "            bar.update(1)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\" Returns a specific item in the dataset\n",
    "        \n",
    "        Arguments:\n",
    "            index: the item's index\n",
    "        \n",
    "        Returns:\n",
    "            a dictionary containing input_ids, attention_mask and label\"\"\"\n",
    "        tokenized_texts, label = self.instances[index]\n",
    "        if self.mode == 'train' or self.mode=='validation':\n",
    "            label_pt = torch.tensor(label)\n",
    "            tokenized_texts['label'] = label_pt\n",
    "\n",
    "        return tokenized_texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''returns length of the dataset'''\n",
    "        return len(self.instances)\n",
    "\n",
    "    def trim_text(self, text: str,tokenizer: transformers.tokenizer, seq_length: int=100):\n",
    "        '''This method gets a string, a tokenizer and length, and trim the given text to\n",
    "        make it equal to the given length. It tries to pick even contexts from both left and right.\n",
    "        \n",
    "        Arguments:\n",
    "            text: the input string\n",
    "            tokenizer\n",
    "            seq_length: the length of final sequence\n",
    "            \n",
    "        Return:\n",
    "            the trimmed text'''\n",
    "        \n",
    "        text = text.replace('[BLANK]', '<mask>')\n",
    "        tokenized_text = tokenizer(text)['input_ids']\n",
    "        blank_spot = tokenized_text.index(tokenizer.mask_token_id)\n",
    "        text_len = len(tokenized_text)\n",
    "        selected_part = []\n",
    "        left_space = blank_spot+1\n",
    "        right_space = text_len - blank_spot -1\n",
    "\n",
    "        if text_len <= seq_length:\n",
    "            return tokenizer.decode(text_len)\n",
    "\n",
    "        if left_space >= int(seq_length/2) and right_space >= int(seq_length/2):\n",
    "            selected_part = tokenized_text[blank_spot-int(seq_length/2)+1:blank_spot+1]\n",
    "            selected_part += tokenized_text[blank_spot+1:blank_spot+1+int(seq_length/2)]\n",
    "        elif left_space > right_space:\n",
    "            remain_space = seq_length - right_space\n",
    "            selected_part = tokenized_text[blank_spot-remain_space+1:blank_spot+1]\n",
    "            selected_part += tokenized_text[blank_spot+1:]\n",
    "        else:\n",
    "            remain_space = seq_length - left_space\n",
    "            selected_part = tokenized_text[:blank_spot+1]\n",
    "            selected_part += tokenized_text[blank_spot+1:blank_spot+1+remain_space]\n",
    "\n",
    "        return tokenizer.decode(selected_part)\n",
    "    \n",
    "            \n",
    "    def extract_choices(self, instance: pd.core.series.Series):\n",
    "        \"\"\"Get an row from the panda dataframe and returns the possible choices.\n",
    "\n",
    "        Arguments:\n",
    "            \n",
    "            instance: a panda dataframe row\n",
    "            \n",
    "        Returns:\n",
    "            list: a list of possible choices\n",
    "        \"\"\"\n",
    "\n",
    "        choices = []\n",
    "        for i in range(1,7):\n",
    "            choice = instance['choice'+str(i)]\n",
    "            if type(choice) == float:\n",
    "#                 choices.append(str(choice))\n",
    "                continue\n",
    "            else:\n",
    "                choices.append(choice)\n",
    "        \n",
    "        choices = self.language_identifier(choices)\n",
    "        return choices\n",
    "    \n",
    "    def language_identifier(self, choices: list):\n",
    "        '''Gets a list of choices, detect their langauge and add the language identifier to each choice\n",
    "        \n",
    "        Arguments:\n",
    "            choices: list\n",
    "        \n",
    "        Returns:\n",
    "            choices with their langauge identifiers: list'''\n",
    "        \n",
    "            option_language = [self.lang_detector.predict(word.lower())[0][0] for word in choices]\n",
    "            counter = Counter(option_language)\n",
    "            language = counter.most_common(1)[0][0]\n",
    "            if language != '__label__en':\n",
    "                modified_choices = [choice+' '+self.en_token for choice in choices]\n",
    "            elif language != '__label__fr':\n",
    "                modified_choices = [choice+' '+self.fr_token for choice in choices]\n",
    "            else: \n",
    "                modified_choices = [choice+' '+self.other_token for choice in choices]\n",
    "            return modified_choices\n",
    "\n",
    "        \n",
    "    def get_label(self, label: str):\n",
    "        '''Gets a string and map it to its numerical label\n",
    "        \n",
    "        Arguments:\n",
    "            label: A string of the label\n",
    "        \n",
    "        Return:\n",
    "            list: a single item list containin the mapped label'''\n",
    "        return{\n",
    "            'choice1': 0,\n",
    "            'choice2': 1,\n",
    "            'choice3': 2,\n",
    "            'choice4': 3,\n",
    "            'choice5': 4,\n",
    "            'choice6': 5,\n",
    "        }[label]\n",
    "    \n",
    "    def length_tokenizer(self):\n",
    "        '''Returns length of tokenizer'''\n",
    "        return(self.total_num)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Unit Test: testing the MultiDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_test = MultiDatset(train_data.iloc[:10], tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=128,\n",
    "                            seq_length = 100)\n",
    "train_dataset_test[:3]['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the training-data and then spliting it into 70% training and 30% validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_shufffled = train_data.sample(frac=1, random_state=2021)\n",
    "train_split , val_split = train_test_split(train_data_shufffled,\n",
    "                                           test_size=0.3,\n",
    "                                           random_state=2021) \n",
    "train_dataset = MultiDatset(train_split, tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=256,\n",
    "                            seq_length = 200)\n",
    "val_dataset = MultiDatset(val_split, tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=256,\n",
    "                            seq_length = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModel():\n",
    "    \n",
    "    \"\"\"The Multi choice model which gets a transformers multi choice model (A language model with a classifier on top)\n",
    "    and train the model on the specified dataset.\n",
    "    \n",
    "    Arguments:\n",
    "        model_name: It is a string specifing which based model you would like to use. options: Longformer, Roberta\n",
    "        model_name: A string poiting out to the location of the model.\n",
    "        total_num_embeddings: the length of tokenizer\n",
    "        is_freezed: if set True, it will freeze all layers except Embeddings and classifier. \"\"\"\n",
    "    \n",
    "    def __init__(self,train_args: transformers.TrainingArguments, total_num_embeddings: int=50265, \n",
    "                 model_name: str = 'Roberta', model_path: str = 'roberta-base', is_freezed: bool=True):\n",
    "        '''Initialize the model'''\n",
    "        model_options = { 'XLM': XLMRobertaForMultipleChoice,\n",
    "                'Roberta': RobertaForMultipleChoice}\n",
    "        \n",
    "        self.model = model_options[model_name].from_pretrained(model_path)\n",
    "        self.model.resize_token_embeddings(total_num_embeddings)\n",
    "        \n",
    "        if is_freezed:\n",
    "\n",
    "            for layer in self.model.roberta.encoder.layer[:]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            for param in [self.model.roberta.pooler.dense.weight, self.model.roberta.pooler.dense.bias]:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        self.train_args = train_args\n",
    "    \n",
    "    def train(self, train_dataset: MultiDatset, val_dataset: MultiDatset):\n",
    "        \n",
    "        \"\"\"This function prepare and start a Transformers.Trainer by getting transformer.TraininggArguments,\n",
    "        And training dataset\n",
    "        \n",
    "        Arguments: \n",
    "            args: Transfromers.TrainingArguments\n",
    "            train_dataset: A MultiC_Dataset dataset containing training instances\n",
    "            eval_dataset: A MultiC_Dataset dataset containing validation instances \n",
    "        Returns:\n",
    "        \n",
    "            It return a Trainer instance. \"\"\"\n",
    "        \n",
    "        trainer = Trainer(model=self.model,\n",
    "                        args=self.train_args,\n",
    "                        train_dataset=train_dataset,         \n",
    "                        eval_dataset=val_dataset,             \n",
    "                        compute_metrics=self.compute_metrics)\n",
    "        return trainer\n",
    "        \n",
    "        \n",
    "    def compute_metrics(self, pred):\n",
    "        '''Calculate the evaluation metrics'''\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        f1micro = f1_score(labels, preds, average='micro')\n",
    "        f1macro =  f1_score(labels, preds, average='macro')\n",
    "        precision = precision_score(labels,preds, average='macro')\n",
    "        recall = recall_score(labels, preds, average='macro')\n",
    "\n",
    "        return {\n",
    "          'accuracy': acc,\n",
    "            'f1micro' : f1micro,\n",
    "            'f1macro' : f1macro,\n",
    "            'precision' : precision,\n",
    "            'recall' : recall,\n",
    "          }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=8,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model= 'f1macro',\n",
    "    gradient_accumulation_steps=8,\n",
    "    seed = 12,   \n",
    ")\n",
    "total_num_embeddings = train_dataset.length_tokenizer()\n",
    "model = MultiModel(training_args, total_num_embeddings, model_name='Roberta', model_path='roberta-base')\n",
    "trainer = model.train(train_dataset, val_dataset)\n",
    "trainer.train()\n",
    "trainer.save_model('./roberta_multiplechoice_onlylastlayer_withlanguageidentifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=8,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model= 'f1macro',\n",
    "    gradient_accumulation_steps=8,\n",
    "    seed = 12,   \n",
    ")\n",
    "total_num_embeddings = train_dataset.length_tokenizer()\n",
    "model = MultiModel(training_args,total_num_embeddings, model_name='Roberta',\n",
    "                   model_path='./roberta_multiplechoice_onlylastlayer_withlanguageidentifier')\n",
    "trainer = model.train(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(results, path):\n",
    "    '''gets a numpy array of predictions and after applying a softmax function, it will write the results.'''\n",
    "    softmax= nn.Softmax(dim=0)\n",
    "    columns = ['idx', 'choice1', 'choice2', 'choice3', 'choice4', 'choice5', 'choice6']\n",
    "    logits = []\n",
    "    for index, row in enumerate(results):\n",
    "#         print(row)\n",
    "        tensor_row = torch.tensor(row, dtype=float)\n",
    "#         print(softmax(tensor_row).tolist())\n",
    "        logits.append([index]+ softmax(tensor_row).tolist())\n",
    "        \n",
    "    with open(path, 'w+', encoding='utf-8') as f:\n",
    "        write = csv.writer(f)\n",
    "        write.writerow(columns)\n",
    "        write.writerows(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./coveo/test.csv')\n",
    "test_dataset = MultiDatset(test_data, tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=256,\n",
    "                            seq_length = 200, mode='test')\n",
    "results = trainer.predict(test_dataset)\n",
    "write_csv(results[0], './results_roberta_multiplechoice_onlylastlayer_withlanguageidentifier.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_langauge(dataframe):\n",
    "    '''gets a dataframe and split it based on 3 languages, En, Fr, and miss-classified   '''\n",
    "    model = fasttext.load_model('./coveo/lid.176.bin')\n",
    "    pbar = tqdm(total = len(dataframe))\n",
    "    fr_dataset = dataframe.copy()\n",
    "    en_dataset = dataframe.copy()\n",
    "    miss_dataset = dataframe.copy()\n",
    "    for i, (index, row) in enumerate(dataframe.iterrows()):\n",
    "        option_language = [model.predict(word.lower())[0][0] for word in \n",
    "                           dataframe.iloc[i][['choice1', 'choice2', 'choice3', 'choice4', 'choice5', 'choice6']]\n",
    "                          if str(word) != 'nan']\n",
    "        counter = Counter(option_language)\n",
    "        language = counter.most_common(1)[0][0]\n",
    "        if language != '__label__en':\n",
    "            en_dataset.drop(index=index, axis=0, inplace=True)\n",
    "        if language != '__label__fr':\n",
    "            fr_dataset.drop(index=index, axis=0, inplace=True)\n",
    "        if language == '__label__fr' or language == '__label__en':\n",
    "            miss_dataset.drop(index=index, axis=0, inplace=True)\n",
    "        pbar.update(1)\n",
    "    return en_dataset, fr_dataset, miss_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dataset, fr_dataset, miss_dataset = split_dataset_langauge()\n",
    "en_multidataset = MultiDatset(en_dataset, tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=256,\n",
    "                            seq_length = 200, mode='validation')\n",
    "fr_multidataset = MultiDatset(fr_dataset, tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=256,\n",
    "                            seq_length = 200, mode='validation')\n",
    "miss_multidataset = MultiDatset(miss_dataset, tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=256,\n",
    "                            seq_length = 200, mode='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(en_multidataset)\n",
    "rainer.predict(fr_multidataset)\n",
    "rainer.predict(miss_multidataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_seq_length(dataframe, tokenizer):\n",
    "    '''Split a dataframe into 4 bins based on the input sequence length'''\n",
    "    \n",
    "    bin1 = []\n",
    "    bin2 = []\n",
    "    bin3 = []\n",
    "    bin4 = []\n",
    "    bar = tqdm(total = len(dataframe))\n",
    "    for idx, (index, row) in enumerate(dataframe.iterrows()):\n",
    "        text = trim_text(row['text'], tokenizer)\n",
    "        length = len(text)\n",
    "        if length < 50:\n",
    "            bin1.append(datframe.iloc[idx].tolist())\n",
    "        elif length >=50 and length <100 :\n",
    "            bin2.append(datframe.iloc[idx].tolist())\n",
    "        elif length >=  100 and length< 150:\n",
    "        \n",
    "            bin3.append(datframe.iloc[idx].tolist())\n",
    "        else: \n",
    "            bin4.append(datframe.iloc[idx].tolist())\n",
    "            \n",
    "        bar.update(1)\n",
    "    return (pd.DataFrame(bin1, columns=dataframe.columns),pd.DataFrame(bin2, columns=dataframe.columns),\n",
    "            pd.DataFrame(bin3, columns=dataframe.columns),pd.DataFrame(bin4, columns=dataframe.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "bin1,bin2,bin3,bin4 = calculate_length(val_split, tokenizer)\n",
    "bin1_multidataset = MultiDatset(bin1, tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=256,\n",
    "                            seq_length = 200, mode='validation')\n",
    "bin2_multidatset = MultiDatset(bin2, tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=256,\n",
    "                            seq_length = 200, mode='validation')\n",
    "bin3_multidatset = MultiDatset(bin3, tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=256,\n",
    "                            seq_length = 200, mode='validation')\n",
    "bin4_multidatset = MultiDatset(bin4, tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=256,\n",
    "                            seq_length = 200, mode='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(bin1_multidataset)\n",
    "trainer.predict(bin2_multidataset)\n",
    "trainer.predict(bin3_multidataset)\n",
    "trainer.predict(bin4_multidataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blank_position(dataframe, tokenizer):\n",
    "    '''Find the position of \"BLANK\" word in the input text and split the dataframe into 4 bins \n",
    "    based on the \"BLANK\" position'''\n",
    "    bar = tqdm(total = len(dataframe))\n",
    "    bin1 = []\n",
    "    bin2 = []\n",
    "    bin3 = []\n",
    "    bin4 = []\n",
    "\n",
    "    for idx, (index, row) in enumerate(dataframe.iterrows()):\n",
    "        text = trim_text(row['text'], tokenizer)\n",
    "        index = text.index(tokenizer.mask_token_id)\n",
    "        \n",
    "        if index < 50:\n",
    "            bin1.append(dataframe.iloc[idx].tolist())\n",
    "        elif index >=50 and index <100 :\n",
    "            bin2.append(dataframe.iloc[idx].tolist())\n",
    "        elif index >=  100 and index< 150:\n",
    "        \n",
    "            bin3.append(dataframe.iloc[idx].tolist())\n",
    "        else: \n",
    "            bin4.append(dataframe.iloc[idx].tolist())\n",
    "            \n",
    "        bar.update(1)\n",
    "    return (pd.DataFrame(bin1, columns=dataframe.columns),pd.DataFrame(bin2, columns=dataframe.columns),\n",
    "            pd.DataFrame(bin3, columns=dataframe.columns),pd.DataFrame(bin4, columns=dataframe.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balnkbin1,balnkbin2,balnkbin3,balnkbin4 = blank_position(val_split, tokenizer)\n",
    "blankbin1_multidataset = MultiDatset(balnkbin1, tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=256,\n",
    "                            seq_length = 200, mode='validation')\n",
    "blankbin2_multidatset = MultiDatset(balnkbin2, tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=256,\n",
    "                            seq_length = 200, mode='validation')\n",
    "blankbin3_multidatset = MultiDatset(balnkbin3, tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=256,\n",
    "                            seq_length = 200, mode='validation')\n",
    "blankbin4_multidatset = MultiDatset(balnkbin4, tokenizer_name='Roberta',\n",
    "                            tokenizer_pth = 'roberta-base', max_length=256,\n",
    "                            seq_length = 200, mode='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(blankbin1_multidatset)\n",
    "trainer.predict(blankbin2_multidatset)\n",
    "trainer.predict(blankbin3_multidatset)\n",
    "trainer.predict(blankbin4_multidatset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
